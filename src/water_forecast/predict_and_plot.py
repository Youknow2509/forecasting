from __future__ import annotations
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from pathlib import Path
from pytorch_forecasting.models import TemporalFusionTransformer
from pytorch_forecasting import TimeSeriesDataSet

from .config import load_config
from .dataio import load_csv, resample_fill
from .features import add_time_features, add_lags_rollings
from .preprocessing import fit_scalers, apply_scalers, train_val_test_split, inverse_transform_target
from .dataset import build_timeseries_datasets


def predict_water_level(cfg_path: str, ckpt_path: str):
    """
    Th·ª±c hi·ªán d·ª± ƒëo√°n m·ª±c n∆∞·ªõc v√† tr·∫£ v·ªÅ k·∫øt qu·∫£.
    
    Returns:
        predictions: numpy array shape [N, H, Q] - d·ª± ƒëo√°n quantiles
        timestamps: list of timestamps t∆∞∆°ng ·ª©ng
        actual_values: numpy array - gi√° tr·ªã th·ª±c t·∫ø (n·∫øu c√≥)
        full_df: DataFrame ƒë·∫ßy ƒë·ªß v·ªõi c√°c features
    """
    cfg = load_config(cfg_path)
    
    # Load v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
    df = load_csv(cfg.paths.data_csv, timezone=cfg.timezone)
    df = resample_fill(df, cfg.frequency, cfg.timezone)
    df = add_time_features(df)
    df = add_lags_rollings(df, "muc_thuong_luu", cfg.lags_hours, cfg.roll_windows_hours, cfg.roll_stats)

    # Chia train/val/test v√† chu·∫©n h√≥a
    tr, va, te = train_val_test_split(df, cfg.split.train_ratio, cfg.split.val_ratio)
    scalers = fit_scalers(tr)
    full = pd.concat([apply_scalers(x, scalers) for x in [tr, va, te]])

    # Sort v√† th√™m time_idx
    full = full.sort_values(["site_id", "timestamp"]).copy()
    full["time_idx"] = full.groupby("site_id").cumcount()
    
    # Fill NaN values trong lag/rolling features
    lag_roll_cols = [c for c in full.columns if c.startswith("muc_thuong_luu_lag_") or c.startswith("muc_thuong_luu_rolling_")]
    for col in lag_roll_cols:
        full[col] = full.groupby("site_id")[col].ffill().bfill()

    # T·∫°o dataset v√† load model
    training, _ = build_timeseries_datasets(full, cfg.enc_len, cfg.dec_len)
    model = TemporalFusionTransformer.load_from_checkpoint(ckpt_path)

    # D·ª± ƒëo√°n
    to_pred = TimeSeriesDataSet.from_dataset(training, full, predict=True, stop_randomization=True)
    dl = to_pred.to_dataloader(train=False, batch_size=cfg.batch_size)
    predictions = model.predict(dl, mode="quantiles")  # [N, H, Q]
    
    # Chuy·ªÉn v·ªÅ numpy
    if hasattr(predictions, 'cpu'):
        predictions = predictions.cpu().numpy()
    
    # INVERSE TRANSFORM: Chuy·ªÉn predictions t·ª´ d·∫°ng chu·∫©n h√≥a v·ªÅ thang ƒëo g·ªëc
    if predictions.ndim == 3:
        # predictions shape: [N, H, Q]
        for i in range(predictions.shape[0]):
            for q in range(predictions.shape[2]):
                predictions[i, :, q] = inverse_transform_target(predictions[i, :, q], scalers, "muc_thuong_luu")
    elif predictions.ndim == 2:
        # predictions shape: [H, Q]
        for q in range(predictions.shape[1]):
            predictions[:, q] = inverse_transform_target(predictions[:, q], scalers, "muc_thuong_luu")
    
    # L·∫•y timestamps cho prediction
    # Gi·∫£ s·ª≠ d·ª± ƒëo√°n t·ª´ th·ªùi ƒëi·ªÉm cu·ªëi c√πng c·ªßa d·ªØ li·ªáu
    last_timestamp = full['timestamp'].max()
    freq = pd.Timedelta(cfg.frequency)
    pred_timestamps = [last_timestamp + freq * (i + 1) for i in range(predictions.shape[1])]
    
    # L·∫•y actual values t·ª´ test set g·ªëc (CH∆ØA scale)
    # te l√† t·ª´ df g·ªëc n√™n kh√¥ng c·∫ßn inverse transform
    actual_values = None
    if len(te) > 0 and 'muc_thuong_luu' in te.columns:
        actual_values = te['muc_thuong_luu'].values[:predictions.shape[1]]
        if len(actual_values) < predictions.shape[1]:
            # Pad v·ªõi NaN n·∫øu kh√¥ng ƒë·ªß
            actual_values = np.concatenate([actual_values, np.full(predictions.shape[1] - len(actual_values), np.nan)])
    
    return predictions, pred_timestamps, actual_values, full, scalers, cfg


def plot_predictions(predictions, timestamps, actual_values=None, save_path="predictions_plot.png"):
    """
    V·∫Ω bi·ªÉu ƒë·ªì d·ª± ƒëo√°n v·ªõi c√°c quantiles.
    
    Args:
        predictions: numpy array shape [N, H, Q] ho·∫∑c [H, Q]
        timestamps: list of timestamps
        actual_values: numpy array - gi√° tr·ªã th·ª±c t·∫ø (optional)
        save_path: ƒë∆∞·ªùng d·∫´n l∆∞u bi·ªÉu ƒë·ªì
    """
    # N·∫øu predictions c√≥ batch dimension, l·∫•y batch ƒë·∫ßu ti√™n
    if predictions.ndim == 3:
        predictions = predictions[0]  # shape [H, Q]
    
    # Gi·∫£ s·ª≠ quantiles l√† [0.1, 0.5, 0.9]
    # ensure lower/upper bounds even if ordering is unexpected
    q = predictions
    if q.ndim == 3:
        q = q[0]
    # q shape [H, Q]
    if q.shape[1] >= 3:
        q10 = q[:, 0]
        q50 = q[:, 1]
        q90 = q[:, 2]
    else:
        # fallback: use min/median/max across axis
        q10 = np.min(q, axis=1)
        q50 = np.median(q, axis=1)
        q90 = np.max(q, axis=1)

    # ensure lower <= median <= upper for plotting
    lower = np.minimum.reduce([q10, q50, q90])
    upper = np.maximum.reduce([q10, q50, q90])
    median = q50
    
    # T·∫°o figure
    fig, ax = plt.subplots(figsize=(15, 6))
    
    # V·∫Ω d·ª± ƒëo√°n
    ax.plot(timestamps, median, label='D·ª± ƒëo√°n (median)', color='tab:blue', linewidth=2.5)
    ax.fill_between(timestamps, lower, upper, alpha=0.3, color='tab:blue', label='Kho·∫£ng tin c·∫≠y (10%-90%)')
    
    # V·∫Ω gi√° tr·ªã th·ª±c t·∫ø n·∫øu c√≥
    if actual_values is not None:
        actual_len = min(len(actual_values), len(timestamps))
        ax.plot(timestamps[:actual_len], actual_values[:actual_len], 
                label='Gi√° tr·ªã th·ª±c t·∫ø', color='tab:red', linewidth=2.5, linestyle='--', marker='o', markersize=5)
    
    # T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh y-axis ƒë·ªÉ zoom v√†o v√πng d·ªØ li·ªáu
    all_values = list(median) + list(lower) + list(upper)
    if actual_values is not None:
        all_values.extend([v for v in actual_values if not np.isnan(v)])
    y_min = np.nanmin(all_values)
    y_max = np.nanmax(all_values)
    y_range = y_max - y_min
    margin = max(y_range * 0.15, 2)  # margin 15% ho·∫∑c t·ªëi thi·ªÉu 2m
    ax.set_ylim(y_min - margin, y_max + margin)
    
    # ƒê·ªãnh d·∫°ng
    ax.set_xlabel('Th·ªùi gian', fontsize=12)
    ax.set_ylabel('M·ª±c n∆∞·ªõc th∆∞·ª£ng l∆∞u (m)', fontsize=12)
    ax.set_title('D·ª± ƒëo√°n M·ª±c N∆∞·ªõc Th∆∞·ª£ng L∆∞u', fontsize=14, fontweight='bold')
    ax.legend(loc='best', fontsize=10)
    ax.grid(True, alpha=0.3, linestyle=':')
    
    # Format x-axis dates
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"‚úì Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {save_path}")
    plt.show()
    
    return fig


def plot_detailed_analysis(predictions, timestamps, actual_values, full_df, save_dir="artifacts"):
    """
    T·∫°o nhi·ªÅu bi·ªÉu ƒë·ªì ph√¢n t√≠ch chi ti·∫øt.
    """
    Path(save_dir).mkdir(parents=True, exist_ok=True)
    
    if predictions.ndim == 3:
        predictions = predictions[0]

    q = predictions
    if q.shape[1] >= 3:
        q10 = q[:, 0]
        q50 = q[:, 1]
        q90 = q[:, 2]
    else:
        q10 = np.min(q, axis=1)
        q50 = np.median(q, axis=1)
        q90 = np.max(q, axis=1)

    lower = np.minimum.reduce([q10, q50, q90])
    upper = np.maximum.reduce([q10, q50, q90])
    median = q50
    
    # 1. Bi·ªÉu ƒë·ªì ch√≠nh v·ªõi historical data + subplot zoom
    fig1 = plt.figure(figsize=(18, 10))
    gs = fig1.add_gridspec(2, 2, height_ratios=[2, 1], width_ratios=[2, 1], hspace=0.3, wspace=0.3)
    ax1 = fig1.add_subplot(gs[0, :])
    
    # L·∫•y 7 ng√†y g·∫ßn nh·∫•t t·ª´ historical data
    hist_hours = 168  # 7 days
    hist_data = full_df.tail(hist_hours)
    hist_timestamps = pd.to_datetime(hist_data['timestamp']).values
    hist_values = hist_data['muc_thuong_luu'].values
    
    # V·∫Ω historical
    ax1.plot(hist_timestamps, hist_values, label='D·ªØ li·ªáu l·ªãch s·ª≠', 
             color='gray', linewidth=2, alpha=0.8)

    # V·∫Ω predictions
    ax1.plot(timestamps, median, label='D·ª± ƒëo√°n (median)', color='tab:blue', linewidth=2.5)
    ax1.fill_between(timestamps, lower, upper, alpha=0.25, color='tab:blue', 
                     label='Kho·∫£ng tin c·∫≠y (10%-90%)')
    
    if actual_values is not None:
        actual_len = min(len(actual_values), len(timestamps))
        ax1.plot(timestamps[:actual_len], actual_values[:actual_len],
                label='Gi√° tr·ªã th·ª±c t·∫ø', color='tab:red', linewidth=2, linestyle='--', marker='o', markersize=3)
    
    ax1.axvline(x=timestamps[0], color='green', linestyle=':', linewidth=2, label='ƒêi·ªÉm d·ª± ƒëo√°n')
    ax1.set_xlabel('Th·ªùi gian', fontsize=13)
    ax1.set_ylabel('M·ª±c n∆∞·ªõc th∆∞·ª£ng l∆∞u (m)', fontsize=13)
    ax1.set_title('D·ª± ƒëo√°n M·ª±c N∆∞·ªõc Th∆∞·ª£ng L∆∞u - Ph√¢n t√≠ch Chi ti·∫øt', fontsize=15, fontweight='bold')
    ax1.legend(loc='best', fontsize=11)
    ax1.grid(True, alpha=0.3, linestyle=':')
    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
    
    # Subplot 1: Zoom v√†o v√πng d·ª± ƒëo√°n (72h ƒë·∫ßu)
    ax2 = fig1.add_subplot(gs[1, 0])
    zoom_hours = min(72, len(timestamps))
    ax2.plot(timestamps[:zoom_hours], median[:zoom_hours], label='D·ª± ƒëo√°n (median)', 
             color='tab:blue', linewidth=2.5, marker='o', markersize=4)
    ax2.fill_between(timestamps[:zoom_hours], lower[:zoom_hours], upper[:zoom_hours], 
                     alpha=0.3, color='tab:blue', label='Kho·∫£ng tin c·∫≠y')
    if actual_values is not None:
        actual_len = min(len(actual_values), zoom_hours)
        ax2.plot(timestamps[:actual_len], actual_values[:actual_len],
                label='Gi√° tr·ªã th·ª±c t·∫ø', color='tab:red', linewidth=2.5, 
                linestyle='--', marker='s', markersize=4)
    
    # Zoom y-axis
    zoom_values = list(median[:zoom_hours]) + list(lower[:zoom_hours]) + list(upper[:zoom_hours])
    if actual_values is not None:
        zoom_values.extend([v for v in actual_values[:zoom_hours] if not np.isnan(v)])
    y_min = np.nanmin(zoom_values)
    y_max = np.nanmax(zoom_values)
    y_range = y_max - y_min
    margin = max(y_range * 0.2, 1)
    ax2.set_ylim(y_min - margin, y_max + margin)
    
    ax2.set_xlabel('Th·ªùi gian (72h ƒë·∫ßu)', fontsize=11)
    ax2.set_ylabel('M·ª±c n∆∞·ªõc (m)', fontsize=11)
    ax2.set_title('Chi ti·∫øt 3 ng√†y ƒë·∫ßu', fontsize=12, fontweight='bold')
    ax2.legend(loc='best', fontsize=9)
    ax2.grid(True, alpha=0.3, linestyle=':')
    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d %H:%M'))
    plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)
    
    # Subplot 2: Hi·ªÉn th·ªã sai s·ªë theo th·ªùi gian
    ax3 = fig1.add_subplot(gs[1, 1])
    if actual_values is not None:
        actual_len = min(len(actual_values), len(median))
        errors = median[:actual_len] - actual_values[:actual_len]
        valid_mask = ~np.isnan(errors)
        timestamps_array = np.array(timestamps[:actual_len])
        ax3.plot(timestamps_array[valid_mask], errors[valid_mask], 
                color='tab:purple', linewidth=2, marker='o', markersize=3)
        ax3.axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)
        ax3.fill_between(timestamps_array[valid_mask], 0, errors[valid_mask], 
                        alpha=0.2, color='tab:purple')
        ax3.set_xlabel('Th·ªùi gian', fontsize=11)
        ax3.set_ylabel('Sai s·ªë (D·ª± ƒëo√°n - Th·ª±c t·∫ø) [m]', fontsize=11)
        ax3.set_title('Ph√¢n t√≠ch Sai s·ªë', fontsize=12, fontweight='bold')
        ax3.grid(True, alpha=0.3, linestyle=':')
        ax3.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)
    
    plt.tight_layout()
    plt.savefig(f"{save_dir}/detailed_forecast.png", dpi=300, bbox_inches='tight')
    print(f"‚úì Bi·ªÉu ƒë·ªì chi ti·∫øt ƒë√£ l∆∞u: {save_dir}/detailed_forecast.png")
    
    # 2. Bi·ªÉu ƒë·ªì uncertainty (ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn)
    # 2. Bi·ªÉu ƒë·ªì uncertainty (ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn)
    fig2, ax2_main = plt.subplots(figsize=(14, 6))
    uncertainty = upper - lower
    ax2_main.plot(timestamps, uncertainty, color='tab:orange', linewidth=2.5, marker='o', markersize=3)
    ax2_main.fill_between(timestamps, 0, uncertainty, alpha=0.25, color='tab:orange')
    
    # Th√™m th√¥ng tin th·ªëng k√™
    mean_unc = np.mean(uncertainty)
    max_unc = np.max(uncertainty)
    ax2_main.axhline(y=mean_unc, color='red', linestyle='--', linewidth=2, 
                    label=f'Trung b√¨nh: {mean_unc:.2f}m')
    ax2_main.text(timestamps[len(timestamps)//2], max_unc * 0.9, 
                 f'Max: {max_unc:.2f}m\nMin: {np.min(uncertainty):.2f}m', 
                 fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    ax2_main.set_xlabel('Th·ªùi gian', fontsize=12)
    ax2_main.set_ylabel('ƒê·ªô kh√¥ng ch·∫Øc ch·∫Øn (m)', fontsize=12)
    ax2_main.set_title('ƒê·ªô Kh√¥ng Ch·∫Øc Ch·∫Øn trong D·ª± ƒêo√°n (Kho·∫£ng tin c·∫≠y 80%)', fontsize=14, fontweight='bold')
    ax2_main.legend(loc='best', fontsize=11)
    ax2_main.grid(True, alpha=0.3, linestyle=':')
    ax2_main.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(f"{save_dir}/uncertainty.png", dpi=300, bbox_inches='tight')
    print(f"‚úì Bi·ªÉu ƒë·ªì ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn ƒë√£ l∆∞u: {save_dir}/uncertainty.png")
    
    # 3. N·∫øu c√≥ actual values, t√≠nh metrics v√† v·∫Ω comparison
    if actual_values is not None:
        actual_len = min(len(actual_values), len(median))
        pred_subset = median[:actual_len]
        actual_subset = actual_values[:actual_len]
        
        # T√≠nh metrics
        mae = np.mean(np.abs(pred_subset - actual_subset))
        rmse = np.sqrt(np.mean((pred_subset - actual_subset) ** 2))
        mape = np.mean(np.abs((pred_subset - actual_subset) / (actual_subset + 1e-8))) * 100
        
        # V·∫Ω scatter plot
        fig3, ax3 = plt.subplots(figsize=(8, 8))
        ax3.scatter(actual_subset, pred_subset, alpha=0.5, s=30)
        
        # ƒê∆∞·ªùng y=x
        min_val = min(actual_subset.min(), pred_subset.min())
        max_val = max(actual_subset.max(), pred_subset.max())
        ax3.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='D·ª± ƒëo√°n ho√†n h·∫£o')
        
        ax3.set_xlabel('Gi√° tr·ªã th·ª±c t·∫ø (m)', fontsize=12)
        ax3.set_ylabel('Gi√° tr·ªã d·ª± ƒëo√°n (m)', fontsize=12)
        ax3.set_title(f'So s√°nh D·ª± ƒëo√°n vs Th·ª±c t·∫ø\nMAE={mae:.3f}m, RMSE={rmse:.3f}m, MAPE={mape:.2f}%', 
                     fontsize=13, fontweight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(f"{save_dir}/comparison.png", dpi=300, bbox_inches='tight')
        print(f"‚úì Bi·ªÉu ƒë·ªì so s√°nh ƒë√£ l∆∞u: {save_dir}/comparison.png")
        
        # In metrics
        print("\n" + "="*50)
        print("üìä METRICS D·ª∞ ƒêO√ÅN:")
        print("="*50)
        print(f"MAE (Mean Absolute Error):     {mae:.4f} m")
        print(f"RMSE (Root Mean Square Error): {rmse:.4f} m")
        print(f"MAPE (Mean Abs % Error):       {mape:.2f} %")
        print("="*50 + "\n")
    
    plt.close('all')


def save_predictions_to_csv(predictions, timestamps, actual_values=None, save_path="predictions.csv"):
    """
    L∆∞u k·∫øt qu·∫£ d·ª± ƒëo√°n ra file CSV.
    """
    if predictions.ndim == 3:
        predictions = predictions[0]
    
    df = pd.DataFrame({
        'timestamp': timestamps,
        'predicted_q10': predictions[:, 0],
        'predicted_median': predictions[:, 1],
        'predicted_q90': predictions[:, 2],
    })
    
    if actual_values is not None:
        actual_len = min(len(actual_values), len(timestamps))
        df['actual_value'] = np.nan
        df.loc[:actual_len-1, 'actual_value'] = actual_values[:actual_len]
    
    df.to_csv(save_path, index=False)
    print(f"‚úì K·∫øt qu·∫£ d·ª± ƒëo√°n ƒë√£ l∆∞u t·∫°i: {save_path}")


def main():
    ap = argparse.ArgumentParser(description='D·ª± ƒëo√°n m·ª±c n∆∞·ªõc v√† t·∫°o bi·ªÉu ƒë·ªì')
    ap.add_argument("--cfg", default="configs/default.yaml", help="ƒê∆∞·ªùng d·∫´n config file")
    ap.add_argument("--ckpt", default="models/tft-best.ckpt", help="ƒê∆∞·ªùng d·∫´n model checkpoint")
    ap.add_argument("--output-dir", default="artifacts", help="Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£")
    ap.add_argument("--simple", action="store_true", help="Ch·ªâ t·∫°o bi·ªÉu ƒë·ªì ƒë∆°n gi·∫£n")
    args = ap.parse_args()

    print("üöÄ B·∫Øt ƒë·∫ßu d·ª± ƒëo√°n m·ª±c n∆∞·ªõc...")
    print("="*60)
    
    # Th·ª±c hi·ªán d·ª± ƒëo√°n
    predictions, timestamps, actual_values, full_df, scalers, cfg = predict_water_level(args.cfg, args.ckpt)
    
    print(f"‚úì Ho√†n th√†nh d·ª± ƒëo√°n!")
    print(f"  - S·ªë b∆∞·ªõc d·ª± ƒëo√°n: {predictions.shape[1]} gi·ªù ({predictions.shape[1]//24} ng√†y)")
    print(f"  - S·ªë quantiles: {predictions.shape[2]}")
    print(f"  - Shape: {predictions.shape}")
    print("="*60 + "\n")
    
    # T·∫°o th∆∞ m·ª•c output
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # L∆∞u predictions ra CSV
    csv_path = output_dir / "predictions.csv"
    save_predictions_to_csv(predictions, timestamps, actual_values, str(csv_path))
    
    # V·∫Ω bi·ªÉu ƒë·ªì
    if args.simple:
        plot_path = output_dir / "predictions_simple.png"
        plot_predictions(predictions, timestamps, actual_values, str(plot_path))
    else:
        print("üìà T·∫°o bi·ªÉu ƒë·ªì ph√¢n t√≠ch chi ti·∫øt...\n")
        plot_detailed_analysis(predictions, timestamps, actual_values, full_df, str(output_dir))
    
    print("\n" + "="*60)
    print("‚úÖ HO√ÄN TH√ÄNH! T·∫•t c·∫£ k·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c:", output_dir)
    print("="*60)


if __name__ == "__main__":
    main()
