# TFT Water Level Forecasting (Multi-Horizon with Quantiles)

Implements your design doc using **PyTorch Forecasting** (TFT), delivering:

-   Data prep & feature engineering (lags, rolling stats, cyclical time)
-   TimeSeriesDataSet for encoder/decoder windows
-   Temporal Fusion Transformer with QuantileLoss `[0.1, 0.5, 0.9]`
-   Evaluation: MAE/RMSE, horizon slices, coverage, CRPS\*
-   Serving API (FastAPI) returning multi-horizon quantile forecasts
-   Dockerfile for deployment
-   Use python 3.11

## Quickstart

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
python -m src.train --cfg configs/default.yaml
python -m src.evaluate --cfg configs/default.yaml --ckpt models/tft-best.ckpt
uvicorn service.app:app --host 0.0.0.0 --port 8000
```

### API

POST `/predict` with JSON:

```json
{
    "history": [
        {
            "timestamp": "2025-01-01T00:00:00Z",
            "site_id": "S1",
            "muc_thuong_luu": 50.1,
            "muc_dang_binh_thuong": 55,
            "muc_chet": 40,
            "luu_luong_den": 120,
            "tong_luong_xa": 60,
            "xa_tran": 0,
            "xa_nha_may": 60,
            "so_cua_xa_sau": 2,
            "so_cua_xa_mat": 1
        }
    ],
    "known_future": [
        {
            "timestamp": "2025-01-08T00:00:00Z",
            "site_id": "S1",
            "muc_thuong_luu": 0,
            "muc_dang_binh_thuong": 55,
            "muc_chet": 40,
            "luu_luong_den": 0,
            "tong_luong_xa": 0,
            "xa_tran": 0,
            "xa_nha_may": 0,
            "so_cua_xa_sau": 2,
            "so_cua_xa_mat": 1,
            "rain_forecast_mm": 5,
            "planned_release_m3s": 50
        }
    ]
}
```

Returns quantiles and horizon arrays.

### Full ETL + Training + Serving Pipeline

```bash
# 1) ETL t·ª´ crawl ‚Üí m·ªôt CSV hu·∫•n luy·ªán chu·∫©n
python -m src.water_forecast.ingest --crawl_dir data/crawl --out data/sample.csv --tz Asia/Bangkok

# 2) Sinh b·ªô train/val/test c·ªë ƒë·ªãnh (c√≥ c·∫£ b·∫£n scaled n·∫øu c·∫ßn)
python -m src.water_forecast.make_splits --cfg configs/default.yaml --out_dir data/processed --format parquet --scaled

# 3) Hu·∫•n luy·ªán nh∆∞ c≈© (train.py v·∫´n c√≥ th·ªÉ t·ª± split n·ªôi b·ªô)
python -m src.water_forecast.train --cfg configs/default.yaml
```

## Testing & Evaluation

### Comprehensive Model Testing

Sau khi hu·∫•n luy·ªán, s·ª≠ d·ª•ng test service ƒë·ªÉ ƒë√°nh gi√° to√†n di·ªán m√¥ h√¨nh:

```bash
# Test m√¥ h√¨nh v·ªõi ƒë·∫ßy ƒë·ªß metrics v√† visualizations
python -m src.water_forecast.test_model --cfg configs/default.yaml

# Ho·∫∑c test v·ªõi checkpoint c·ª• th·ªÉ
python -m src.water_forecast.test_model --cfg configs/default.yaml --ckpt models/tft-best.ckpt

# Ch·ªâ ƒë·ªãnh s·ªë l∆∞·ª£ng sample predictions ƒë·ªÉ plot
python -m src.water_forecast.test_model --cfg configs/default.yaml --samples 10
```

Test service s·∫Ω t·∫°o ra:

-   üìä **metrics.json**: C√°c metrics ƒë√°nh gi√° chi ti·∫øt
-   üìÑ **metrics.txt**: B√°o c√°o metrics d·∫°ng text d·ªÖ ƒë·ªçc
-   üìà **forecast_samples.png**: D·ª± ƒëo√°n m·∫´u v·ªõi uncertainty bands
-   üìâ **error_analysis.png**: Ph√¢n t√≠ch ph√¢n b·ªë l·ªói
-   üìä **coverage_analysis.png**: Ph√¢n t√≠ch ƒë·ªô bao ph·ªß c·ªßa prediction intervals

### Metrics ƒë∆∞·ª£c t√≠nh to√°n

#### Overall Performance

-   **MAE**: Mean Absolute Error
-   **RMSE**: Root Mean Squared Error
-   **MAPE**: Mean Absolute Percentage Error
-   **Coverage_80%**: T·ª∑ l·ªá gi√° tr·ªã th·ª±c n·∫±m trong [Q10, Q90]
-   **Interval_Width**: ƒê·ªô r·ªông trung b√¨nh c·ªßa prediction interval
-   **Sharpness**: ƒê·ªô bi·∫øn thi√™n c·ªßa interval width

#### Horizon-wise Performance

-   **MAE_24h**: MAE cho 24 gi·ªù ƒë·∫ßu
-   **RMSE_24h**: RMSE cho 24 gi·ªù ƒë·∫ßu
-   **MAE_7d**: MAE cho 7 ng√†y ƒë·∫ßu
-   **RMSE_7d**: RMSE cho 7 ng√†y ƒë·∫ßu
-   **MAE_14d**: MAE cho 14 ng√†y ƒë·∫ßu
-   **RMSE_14d**: RMSE cho 14 ng√†y ƒë·∫ßu

#### Quantile Loss

-   **Pinball_Q10**: Pinball loss cho quantile 0.1
-   **Pinball_Q50**: Pinball loss cho quantile 0.5 (median)
-   **Pinball_Q90**: Pinball loss cho quantile 0.9
-   **Avg_Pinball**: Trung b√¨nh pinball loss

### Evaluate Script (Legacy)

Script evaluate c≈© v·∫´n c√≥ s·∫µn:

```bash
python -m src.water_forecast.evaluate --cfg configs/default.yaml
```

### Unit Tests

Ch·∫°y unit tests ƒë·ªÉ ki·ªÉm tra c√°c th√†nh ph·∫ßn ri√™ng l·∫ª:

```bash
# Ch·∫°y t·∫•t c·∫£ tests
pytest tests/

# Ch·∫°y test c·ª• th·ªÉ
pytest tests/test_model.py -v
pytest tests/test_ingest.py -v
pytest tests/test_preprocessing.py -v

# Ch·∫°y v·ªõi coverage
pytest tests/ --cov=src/water_forecast --cov-report=html
```

### Prediction CLI

D·ª± ƒëo√°n nhanh t·ª´ command line:

```bash
python -m src.water_forecast.predict_cli --cfg configs/default.yaml --ckpt models/tft-best.ckpt
```

## M√¥ h√¨nh Temporal Fusion Transformer (TFT)

### T·ªïng quan

D·ª± √°n s·ª≠ d·ª•ng m√¥ h√¨nh **Temporal Fusion Transformer (TFT)** - m·ªôt ki·∫øn tr√∫c deep learning ti√™n ti·∫øn ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho b√†i to√°n d·ª± b√°o chu·ªói th·ªùi gian ƒëa bi·∫øn. TFT k·∫øt h·ª£p c√°c c∆° ch·∫ø attention v·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω c√°c bi·∫øn ƒë·ªông th·ªùi gian ph·ª©c t·∫°p.

### Ki·∫øn tr√∫c m√¥ h√¨nh

#### 1. **C√°c th√†nh ph·∫ßn ch√≠nh**

-   **Variable Selection Networks (VSN)**: T·ª± ƒë·ªông ch·ªçn l·ªçc c√°c bi·∫øn ƒë·∫ßu v√†o quan tr·ªçng nh·∫•t
-   **Gated Residual Networks (GRN)**: X·ª≠ l√Ω th√¥ng tin v·ªõi kh·∫£ nƒÉng h·ªçc phi tuy·∫øn t√≠nh
-   **Multi-head Attention**: H·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá ph·ª• thu·ªôc th·ªùi gian d√†i h·∫°n
-   **Quantile Regression**: D·ª± ƒëo√°n nhi·ªÅu quantiles ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn

#### 2. **C·∫•u h√¨nh m√¥ h√¨nh**

```python
# C√°c tham s·ªë ch√≠nh c·ªßa TFT
hidden_size = 160              # K√≠ch th∆∞·ªõc hidden layer
attention_heads = 4            # S·ªë l∆∞·ª£ng attention heads
dropout = 0.1                  # T·ª∑ l·ªá dropout ƒë·ªÉ tr√°nh overfitting
learning_rate = 1e-3           # T·ªëc ƒë·ªô h·ªçc
quantiles = (0.1, 0.5, 0.9)   # C√°c quantile ƒë·ªÉ d·ª± ƒëo√°n
```

#### 3. **C√°c l·ªõp x·ª≠ l√Ω**

##### a. **Input Layer**

-   Nh·∫≠n ƒë·∫ßu v√†o l√† chu·ªói th·ªùi gian v·ªõi nhi·ªÅu bi·∫øn (multivariate time series)
-   X·ª≠ l√Ω c√°c lo·∫°i bi·∫øn:
    -   **Static covariates**: C√°c bi·∫øn kh√¥ng ƒë·ªïi theo th·ªùi gian (vd: ID tr·∫°m ƒëo)
    -   **Time-varying known**: C√°c bi·∫øn bi·∫øt tr∆∞·ªõc (vd: th·ªùi gian trong ng√†y, th√°ng)
    -   **Time-varying unknown**: C√°c bi·∫øn ch·ªâ bi·∫øt trong qu√° kh·ª© (vd: l∆∞·ª£ng m∆∞a th·ª±c t·∫ø)

##### b. **Variable Selection Network**

-   T·ª± ƒë·ªông ƒë√°nh tr·ªçng s·ªë cho t·ª´ng bi·∫øn ƒë·∫ßu v√†o
-   Gi√∫p m√¥ h√¨nh t·∫≠p trung v√†o c√°c bi·∫øn quan tr·ªçng nh·∫•t
-   C·∫£i thi·ªán kh·∫£ nƒÉng gi·∫£i th√≠ch c·ªßa m√¥ h√¨nh

##### c. **LSTM Encoder-Decoder**

-   **Encoder**: X·ª≠ l√Ω d·ªØ li·ªáu qu√° kh·ª© (historical data)
-   **Decoder**: X·ª≠ l√Ω c√°c bi·∫øn known trong t∆∞∆°ng lai
-   K·∫øt h·ª£p th√¥ng tin t·ª´ c·∫£ qu√° kh·ª© v√† t∆∞∆°ng lai ƒë√£ bi·∫øt

##### d. **Multi-head Attention Layer**

-   H·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá ph·ª• thu·ªôc gi·ªØa c√°c b∆∞·ªõc th·ªùi gian
-   Cho ph√©p m√¥ h√¨nh "ch√∫ √Ω" ƒë·∫øn c√°c th·ªùi ƒëi·ªÉm quan tr·ªçng trong qu√° kh·ª©
-   S·ª≠ d·ª•ng 4 attention heads ƒë·ªÉ h·ªçc nhi·ªÅu pattern kh√°c nhau

##### e. **Gated Residual Network (GRN)**

-   X·ª≠ l√Ω th√¥ng tin phi tuy·∫øn t√≠nh
-   C√≥ c∆° ch·∫ø gating ƒë·ªÉ ki·ªÉm so√°t lu·ªìng th√¥ng tin
-   K·∫øt n·ªëi residual gi√∫p hu·∫•n luy·ªán m√¥ h√¨nh s√¢u h∆°n

##### f. **Output Layer - Quantile Regression**

-   D·ª± ƒëo√°n 3 quantiles: 10%, 50% (median), 90%
-   **Q10 (0.1)**: Gi·ªõi h·∫°n d∆∞·ªõi - k·ªãch b·∫£n l·∫°c quan
-   **Q50 (0.5)**: D·ª± ƒëo√°n trung b√¨nh - k·ªãch b·∫£n c√≥ kh·∫£ nƒÉng nh·∫•t
-   **Q90 (0.9)**: Gi·ªõi h·∫°n tr√™n - k·ªãch b·∫£n bi quan
-   Cho ph√©p ƒë√°nh gi√° ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn c·ªßa d·ª± ƒëo√°n

### C√°ch th·ª©c d·ª± ƒëo√°n

#### 1. **Qu√° tr√¨nh d·ª± ƒëo√°n**

```python
# B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o
# - Context window: N b∆∞·ªõc th·ªùi gian trong qu√° kh·ª©
# - Prediction horizon: M b∆∞·ªõc th·ªùi gian c·∫ßn d·ª± ƒëo√°n

# B∆∞·ªõc 2: Variable Selection
# M√¥ h√¨nh ƒë√°nh gi√° t·∫ßm quan tr·ªçng c·ªßa t·ª´ng bi·∫øn

# B∆∞·ªõc 3: LSTM Encoding
# M√£ h√≥a th√¥ng tin t·ª´ chu·ªói qu√° kh·ª©

# B∆∞·ªõc 4: Temporal Fusion
# K·∫øt h·ª£p th√¥ng tin qu√° kh·ª© v·ªõi c√°c bi·∫øn known t∆∞∆°ng lai

# B∆∞·ªõc 5: Multi-head Attention
# T·∫≠p trung v√†o c√°c th·ªùi ƒëi·ªÉm quan tr·ªçng

# B∆∞·ªõc 6: Quantile Prediction
# D·ª± ƒëo√°n 3 gi√° tr·ªã cho m·ªói b∆∞·ªõc th·ªùi gian
```

#### 2. **V√≠ d·ª• s·ª≠ d·ª•ng**

```python
from water_forecast.tft_model import make_tft

# T·∫°o m√¥ h√¨nh t·ª´ dataset
model = make_tft(
    training_dataset,
    hidden_size=160,
    attention_heads=4,
    dropout=0.1,
    learning_rate=1e-3,
    quantiles=(0.1, 0.5, 0.9)
)

# Hu·∫•n luy·ªán
trainer.fit(model, train_loader, val_loader)

# D·ª± ƒëo√°n
predictions = model.predict(test_data)

# K·∫øt qu·∫£: dictionary ch·ª©a c√°c quantiles
# predictions["prediction"]: shape (batch_size, prediction_length, 3)
# [:, :, 0]: quantile 0.1 (gi·ªõi h·∫°n d∆∞·ªõi)
# [:, :, 1]: quantile 0.5 (d·ª± ƒëo√°n trung b√¨nh)
# [:, :, 2]: quantile 0.9 (gi·ªõi h·∫°n tr√™n)
```

### Loss Function - Quantile Loss

M√¥ h√¨nh s·ª≠ d·ª•ng **Quantile Loss** ƒë·ªÉ t·ªëi ∆∞u h√≥a:

```python
QuantileLoss = Œ£ max[q(y - ≈∑), (q-1)(y - ≈∑)]
```

Trong ƒë√≥:

-   `y`: Gi√° tr·ªã th·ª±c t·∫ø
-   `≈∑`: Gi√° tr·ªã d·ª± ƒëo√°n
-   `q`: Quantile (0.1, 0.5, 0.9)

**∆Øu ƒëi·ªÉm:**

-   Cho ph√©p d·ª± ƒëo√°n kho·∫£ng tin c·∫≠y, kh√¥ng ch·ªâ gi√° tr·ªã ƒëi·ªÉm
-   Ph√π h·ª£p v·ªõi c√°c t√¨nh hu·ªëng c·∫ßn ƒë√°nh gi√° r·ªßi ro
-   X·ª≠ l√Ω t·ªët v·ªõi d·ªØ li·ªáu c√≥ outliers

### ∆Øu ƒëi·ªÉm c·ªßa TFT

1. **Kh·∫£ nƒÉng gi·∫£i th√≠ch cao**

    - Variable importance: Bi·∫øt bi·∫øn n√†o quan tr·ªçng
    - Attention weights: Hi·ªÉu m√¥ h√¨nh t·∫≠p trung v√†o th·ªùi ƒëi·ªÉm n√†o

2. **X·ª≠ l√Ω d·ªØ li·ªáu ph·ª©c t·∫°p**

    - Nhi·ªÅu bi·∫øn ƒë·∫ßu v√†o (multivariate)
    - C√°c bi·∫øn v·ªõi t√≠nh ch·∫•t kh√°c nhau (static, time-varying)
    - Missing data handling

3. **D·ª± ƒëo√°n kho·∫£ng tin c·∫≠y**

    - Quantile prediction cho ph√©p ƒë√°nh gi√° ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn
    - H·ªØu √≠ch cho ra quy·∫øt ƒë·ªãnh trong ƒëi·ªÅu ki·ªán b·∫•t ƒë·ªãnh

4. **Hi·ªáu su·∫•t cao**
    - K·∫øt h·ª£p LSTM v√† Attention hi·ªáu qu·∫£
    - H·ªçc ƒë∆∞·ª£c c·∫£ ph·ª• thu·ªôc ng·∫Øn h·∫°n v√† d√†i h·∫°n

### Hyperparameters ch√≠nh

| Tham s·ªë                      | Gi√° tr·ªã m·∫∑c ƒë·ªãnh | M√¥ t·∫£                                      |
| ---------------------------- | ---------------- | ------------------------------------------ |
| `hidden_size`                | 160              | K√≠ch th∆∞·ªõc c·ªßa c√°c hidden layers           |
| `attention_heads`            | 4                | S·ªë l∆∞·ª£ng attention heads                   |
| `dropout`                    | 0.1              | T·ª∑ l·ªá dropout (0-1)                        |
| `learning_rate`              | 1e-3             | T·ªëc ƒë·ªô h·ªçc ban ƒë·∫ßu                         |
| `quantiles`                  | (0.1, 0.5, 0.9)  | C√°c quantile ƒë·ªÉ d·ª± ƒëo√°n                    |
| `log_interval`               | 50               | T·∫ßn su·∫•t log metrics                       |
| `reduce_on_plateau_patience` | 4                | S·ªë epochs ch·ªù tr∆∞·ªõc khi gi·∫£m learning rate |

### Monitoring & Callbacks

M√¥ h√¨nh s·ª≠ d·ª•ng c√°c callbacks c·ªßa PyTorch Lightning:

-   **EarlyStopping**: D·ª´ng training khi validation loss kh√¥ng c·∫£i thi·ªán
-   **ModelCheckpoint**: L∆∞u best model theo validation loss
-   **LearningRateMonitor**: Theo d√µi learning rate schedule
-   **ReduceLROnPlateau**: T·ª± ƒë·ªông gi·∫£m learning rate khi loss plateau

### T√†i li·ªáu tham kh·∫£o

-   [PyTorch Forecasting Documentation](https://pytorch-forecasting.readthedocs.io/)
-   [Temporal Fusion Transformers Paper](https://arxiv.org/abs/1912.09363)
-   [Lightning Documentation](https://lightning.ai/docs/pytorch/stable/)
